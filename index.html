<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chain-of-Thought Prompting: Eliciting Reasoning in Large Language Models</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        .header {
            text-align: center;
            margin-bottom: 40px;
            color: white;
        }

        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }

        .header p {
            font-size: 1.2rem;
            opacity: 0.9;
        }

        .section {
            background: white;
            border-radius: 15px;
            padding: 30px;
            margin-bottom: 30px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
            transition: transform 0.3s ease;
        }

        .section:hover {
            transform: translateY(-5px);
        }

        .section h2 {
            color: #4a5568;
            margin-bottom: 20px;
            font-size: 1.8rem;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
        }

        .alternatives {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }

        .alternative-card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid #667eea;
        }

        .alternative-card h3 {
            color: #2d3748;
            margin-bottom: 10px;
        }

        .task-cards {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }

        .task-card {
            background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 50%, #fecfef 100%);
            border-radius: 15px;
            padding: 25px;
            cursor: pointer;
            transition: all 0.3s ease;
            position: relative;
            overflow: hidden;
        }

        .task-card:hover {
            transform: scale(1.02);
            box-shadow: 0 15px 40px rgba(0,0,0,0.15);
        }

        .task-card h3 {
            color: #2d3748;
            margin-bottom: 15px;
            font-size: 1.4rem;
        }

        .task-card-summary {
            color: #4a5568;
            margin-bottom: 15px;
        }

        .expand-icon {
            position: absolute;
            top: 20px;
            right: 20px;
            font-size: 1.5rem;
            color: #667eea;
            transition: transform 0.3s ease;
        }

        .task-card.expanded .expand-icon {
            transform: rotate(180deg);
        }

        .task-details {
            display: none;
            margin-top: 20px;
            padding-top: 20px;
            border-top: 2px solid rgba(255,255,255,0.3);
            animation: fadeIn 0.3s ease;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(-10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .benchmark-list {
            background: rgba(255,255,255,0.7);
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }

        .benchmark-list h4 {
            color: #2d3748;
            margin-bottom: 10px;
        }

        .benchmark-list ul {
            list-style-type: none;
            padding-left: 0;
        }

        .benchmark-list li {
            background: rgba(255,255,255,0.5);
            margin: 5px 0;
            padding: 8px 12px;
            border-radius: 5px;
            border-left: 3px solid #667eea;
        }

        .prompt-example {
            background: #2d3748;
            color: white;
            padding: 20px;
            border-radius: 8px;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
        }

        .prompt-example h4 {
            color: #a0aec0;
            margin-bottom: 10px;
        }

        .results-section {
            background: rgba(255,255,255,0.7);
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }

        .results-chart {
            text-align: center;
            margin: 20px 0;
            padding: 15px;
            background: rgba(255,255,255,0.9);
            border-radius: 10px;
        }

        .results-chart img {
            transition: transform 0.3s ease;
        }

        .results-chart img:hover {
            transform: scale(1.05);
        }

        .special-sections {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 20px;
            margin-top: 30px;
        }

        .special-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .special-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 40px rgba(0,0,0,0.2);
        }

        .special-details {
            display: none;
            margin-top: 20px;
            padding-top: 20px;
            border-top: 2px solid rgba(255,255,255,0.3);
        }

        .btn {
            background: #667eea;
            color: white;
            padding: 10px 20px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            transition: background 0.3s ease;
        }

        .btn:hover {
            background: #5a67d8;
        }

        .highlight {
            background: #fef5e7;
            padding: 15px;
            border-left: 4px solid #ed8936;
            border-radius: 5px;
            margin: 15px 0;
        }

        .error-analysis {
            background: #fed7d7;
            border: 1px solid #fc8181;
            border-radius: 8px;
            padding: 15px;
            margin: 15px 0;
        }

        .success-analysis {
            background: #c6f6d5;
            border: 1px solid #68d391;
            border-radius: 8px;
            padding: 15px;
            margin: 15px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Chain-of-Thought Prompting</h1>
            <p>Eliciting Reasoning in Large Language Models</p>
            <p><em>Wei et al., Google Research, NeurIPS 2022</em></p>
        </div>

        <!-- The Ability to Reason: Alternatives -->
        <div class="section">
            <h2>üß† The Ability to Reason: Current Approaches</h2>
            <p>Before Chain-of-Thought prompting, researchers used several approaches to enable reasoning in language models:</p>
            
            <div class="alternatives">
                <div class="alternative-card">
                    <h3>üèóÔ∏è Training from Scratch</h3>
                    <p>Train models specifically for reasoning tasks by providing labeled reasoning data</p>
                    <strong>Limitation:</strong> Requires extensive labeled datasets and computational resources
                </div>
                
                <div class="alternative-card">
                    <h3>‚öôÔ∏è Fine-tuning + Neuro-Symbolic</h3>
                    <p>Fine-tune pretrained models with formal language representations and symbolic reasoning</p>
                    <strong>Limitation:</strong> Task-specific, requires domain expertise and additional training
                </div>
                
                <div class="alternative-card">
                    <h3>üìù In-Context Few-Shot Learning</h3>
                    <p>Standard prompting with input/output examples</p>
                    <strong>Limitation:</strong> Poor performance on complex reasoning tasks, no intermediate steps
                </div>
            </div>
        </div>

        <!-- Limitations of Previous Approaches -->
        <div class="section">
            <h2>‚ö†Ô∏è Limitations of Previous Approaches</h2>
            <div class="highlight">
                <h3>Key Problems:</h3>
                <ul>
                    <li><strong>Costly Training:</strong> Creating large datasets of high-quality rationales is expensive and time-consuming</li>
                    <li><strong>Poor Scaling:</strong> Traditional few-shot prompting doesn't improve substantially with increasing model scale</li>
                    <li><strong>Task-Specific:</strong> Each approach requires separate model checkpoints for different tasks</li>
                    <li><strong>No Interpretability:</strong> Models provide answers without showing reasoning process</li>
                </ul>
            </div>
        </div>

        <!-- Benefits of Chain-of-Thought -->
        <div class="section">
            <h2>‚ú® Benefits of Chain-of-Thought Prompting</h2>
            <div class="alternatives">
                <div class="alternative-card">
                    <h3>üîó Problem Decomposition</h3>
                    <p>Allows models to break down multi-step problems into intermediate steps, allocating more computation to complex reasoning</p>
                </div>
                
                <div class="alternative-card">
                    <h3>üëÅÔ∏è Interpretability</h3>
                    <p>Provides a window into the model's behavior, showing how it arrived at an answer and enabling debugging</p>
                </div>
                
                <div class="alternative-card">
                    <h3>üåê Broad Applicability</h3>
                    <p>Works across math, commonsense reasoning, and symbolic manipulation - any task humans can solve via language</p>
                </div>
                
                <div class="alternative-card">
                    <h3>üöÄ Zero Training</h3>
                    <p>Can be elicited in large off-the-shelf models simply by including chain-of-thought examples in prompts</p>
                </div>
            </div>
        </div>

        <!-- Experiment Overview -->
        <div class="section">
            <h2>üî¨ Experimental Setup</h2>
            <div class="alternatives">
                <div class="alternative-card">
                    <h3>ü§ñ Models Tested</h3>
                    <ul>
                        <li><strong>GPT-3:</strong> 350M, 1.3B, 6.7B, 175B parameters</li>
                        <li><strong>LaMDA:</strong> 422M, 2B, 8B, 68B, 137B parameters</li>
                        <li><strong>PaLM:</strong> 8B, 62B, 540B parameters</li>
                        <li><strong>UL2:</strong> 20B parameters</li>
                        <li><strong>Codex:</strong> code-davinci-002</li>
                    </ul>
                </div>
                
                <div class="alternative-card">
                    <h3>üìä Benchmark Categories</h3>
                    <ul>
                        <li><strong>Arithmetic:</strong> GSM8K, SVAMP, ASDiv, AQuA, MAWPS</li>
                        <li><strong>Commonsense:</strong> CSQA, StrategyQA, Date Understanding, Sports Understanding</li>
                        <li><strong>Symbolic:</strong> Last Letter Concatenation, Coin Flip tracking</li>
                    </ul>
                </div>
                
                <div class="alternative-card">
                    <h3>üß™ Testing Methodology</h3>
                    <ul>
                        <li>8 manually composed few-shot exemplars</li>
                        <li>Greedy decoding for consistency</li>
                        <li>Multiple random seeds for robustness</li>
                        <li>Comparison with standard prompting baseline</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Task Deep Dives -->
        <div class="section">
            <h2>üìã Task Deep Dives</h2>
            <p>Click on each card to expand and explore the methodology, benchmarks, and results for each reasoning task:</p>
            
            <div class="task-cards">
                <!-- Arithmetic Reasoning Card -->
                <div class="task-card" onclick="toggleCard('arithmetic')">
                    <span class="expand-icon">‚ñº</span>
                    <h3>üî¢ Arithmetic Reasoning</h3>
                    <div class="task-card-summary">
                        Math word problems requiring multi-step calculations and logical reasoning
                    </div>
                    
                    <div class="task-details" id="arithmetic-details">
                        <div class="benchmark-list">
                            <h4>Benchmarks Used:</h4>
                            <ul>
                                <li><strong>GSM8K:</strong> Grade school math word problems (1,319 examples)</li>
                                <li><strong>SVAMP:</strong> Math problems with varying structures (1,000 examples)</li>
                                <li><strong>ASDiv:</strong> Diverse math word problems (2,096 examples)</li>
                                <li><strong>AQuA:</strong> Algebraic word problems (254 examples)</li>
                                <li><strong>MAWPS:</strong> Math word problem repository (various subsets)</li>
                            </ul>
                        </div>
                        
                        <div class="prompt-example">
                            <h4>Example Chain-of-Thought Prompt:</h4>
                            <div style="margin-top: 10px;">
                                <strong>Q:</strong> Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?<br><br>
                                <strong>A:</strong> Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.
                            </div>
                        </div>
                        
                        <div class="results-section">
                            <h4>Key Results:</h4>
                            <ul>
                                <li><strong>PaLM 540B GSM8K:</strong> 17.9% ‚Üí 56.9% (+39% improvement)</li>
                                <li><strong>New State-of-the-Art:</strong> Surpassed fine-tuned GPT-3 with verifier</li>
                                <li><strong>Emergent Ability:</strong> Only works with models ‚â•100B parameters</li>
                            </ul>
                        </div>
                        
                        <div class="results-chart">
                            <img src="https://irivelez.github.io/chain-of-thought/Figure4.png" 
                                 alt="Chain-of-thought prompting performance across model sizes" 
                                 style="width: 100%; max-width: 600px; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                            <p style="text-align: center; margin-top: 10px; font-style: italic; color: #666;">
                                Figure 4: Chain-of-thought prompting enables large language models to solve challenging math problems. 
                                Performance is an emergent ability of increasing model scale.
                            </p>
                        </div>
                        
                        <div class="error-analysis">
                            <h4>üìã Manual Review Analysis (LaMDA 137B, 50 correct answers):</h4>
                            <ul>
                                <li><strong>48 (96%) were genuinely correct</strong> with proper reasoning</li>
                                <li><strong>2 (4%) arrived at correct answer by coincidence</strong></li>
                            </ul>
                        </div>
                        
                        <div class="success-analysis">
                            <h4>üìã Error Analysis (50 incorrect answers):</h4>
                            <ul>
                                <li><strong>8%:</strong> Calculator errors only (reasoning was correct)</li>
                                <li><strong>16%:</strong> Symbol mapping errors</li>
                                <li><strong>22%:</strong> One step missing</li>
                                <li><strong>54%:</strong> Major semantic understanding errors</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <!-- Commonsense Reasoning Card -->
                <div class="task-card" onclick="toggleCard('commonsense')">
                    <span class="expand-icon">‚ñº</span>
                    <h3>üß© Commonsense Reasoning</h3>
                    <div class="task-card-summary">
                        Reasoning about physical and human interactions using general background knowledge
                    </div>
                    
                    <div class="task-details" id="commonsense-details">
                        <div class="benchmark-list">
                            <h4>Benchmarks Used:</h4>
                            <ul>
                                <li><strong>CSQA:</strong> CommonsenseQA - questions about the world with complex semantics</li>
                                <li><strong>StrategyQA:</strong> Multi-hop strategy inference questions</li>
                                <li><strong>Date Understanding:</strong> Inferring dates from context (BIG-bench)</li>
                                <li><strong>Sports Understanding:</strong> Plausibility of sports-related sentences (BIG-bench)</li>
                                <li><strong>SayCan:</strong> Mapping natural language to robot action sequences</li>
                            </ul>
                        </div>
                        
                        <div class="prompt-example">
                            <h4>Example Chain-of-Thought Prompt:</h4>
                            <div style="margin-top: 10px;">
                                <strong>Q:</strong> Yes or no: Would a pear sink in water?<br><br>
                                <strong>A:</strong> The density of a pear is about 0.6 g/cm¬≥, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
                            </div>
                        </div>
                        
                        <div class="results-section">
                            <h4>Key Results:</h4>
                            <ul>
                                <li><strong>StrategyQA:</strong> New state-of-the-art (75.6% vs 69.4% prior best)</li>
                                <li><strong>Sports Understanding:</strong> 95.4% accuracy (beat human enthusiast at 84%)</li>
                                <li><strong>Date Understanding:</strong> Significant improvements across all model sizes</li>
                                <li><strong>SayCan:</strong> Better robot instruction following</li>
                            </ul>
                        </div>
                        
                        <div class="results-chart">
                            <img src="https://irivelez.github.io/chain-of-thought/Figure%207.png" 
                                 alt="Chain-of-thought prompting commonsense reasoning results" 
                                 style="width: 100%; max-width: 600px; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                            <p style="text-align: center; margin-top: 10px; font-style: italic; color: #666;">
                                Figure 7: Chain-of-thought prompting improves commonsense reasoning abilities across different tasks and model scales.
                            </p>
                        </div>
                    </div>
                </div>

                <!-- Symbolic Reasoning Card -->
                <div class="task-card" onclick="toggleCard('symbolic')">
                    <span class="expand-icon">‚ñº</span>
                    <h3>üîÑ Symbolic Reasoning</h3>
                    <div class="task-card-summary">
                        Following systematic procedures and generalizing to longer sequences
                    </div>
                    
                    <div class="task-details" id="symbolic-details">
                        <div class="benchmark-list">
                            <h4>Tasks Designed:</h4>
                            <ul>
                                <li><strong>Last Letter Concatenation:</strong> Extract and combine last letters of words in names</li>
                                <li><strong>Coin Flip Tracking:</strong> Track coin state through series of flips by different people</li>
                            </ul>
                        </div>
                        
                        <div class="prompt-example">
                            <h4>Example Chain-of-Thought Prompt:</h4>
                            <div style="margin-top: 10px;">
                                <strong>Q:</strong> Take the last letters of the words in "Elon Musk" and concatenate them.<br><br>
                                <strong>A:</strong> The last letter of "Elon" is "n". The last letter of "Musk" is "k". Concatenating them is "nk". The answer is nk.
                            </div>
                        </div>
                        
                        <div class="results-section">
                            <h4>Key Results:</h4>
                            <ul>
                                <li><strong>In-Domain:</strong> Near 100% accuracy with PaLM 540B</li>
                                <li><strong>Length Generalization:</strong> Successfully handles longer sequences than training examples</li>
                                <li><strong>Standard Prompting Fails:</strong> 0% accuracy on out-of-domain tasks</li>
                                <li><strong>Chain-of-Thought Enables:</strong> Systematic reasoning on novel inputs</li>
                            </ul>
                        </div>
                        
                        <div class="results-chart">
                            <img src="https://irivelez.github.io/chain-of-thought/Figure%208.png" 
                                 alt="Chain-of-thought prompting symbolic reasoning and length generalization" 
                                 style="width: 100%; max-width: 600px; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                            <p style="text-align: center; margin-top: 10px; font-style: italic; color: #666;">
                                Figure 8: Chain-of-thought prompting facilitates generalization to longer sequences in symbolic reasoning tasks.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Special Considerations -->
        <div class="section">
            <h2>üéØ Special Considerations</h2>
            
            <div class="special-sections">
                <div class="special-card" onclick="toggleSpecial('scale')">
                    <h3>üìà Why Does Chain-of-Thought Improve with Scale?</h3>
                    <p>Understanding the relationship between model size and reasoning ability</p>
                    
                    <div class="special-details" id="scale-details">
                        <div class="highlight">
                            <h4>üß† Hypothesis: Emergent Reasoning Abilities</h4>
                            <p>Language models acquire semantic understanding and logical reasoning skills as a function of model scale. This involves multiple emergent abilities:</p>
                            <ul>
                                <li><strong>Semantic Understanding:</strong> Better comprehension of problem context</li>
                                <li><strong>Symbol Mapping:</strong> Correct translation between concepts and numbers</li>
                                <li><strong>Arithmetic Ability:</strong> Basic mathematical operations</li>
                                <li><strong>Staying on Topic:</strong> Maintaining coherent reasoning chains</li>
                                <li><strong>Faithfulness:</strong> Following logical steps consistently</li>
                            </ul>
                        </div>
                        
                        <div class="error-analysis">
                            <h4>üìä Error Analysis: PaLM 62B ‚Üí 540B Improvements</h4>
                            <p>Scaling fixed substantial portions of errors in all categories:</p>
                            <ul>
                                <li><strong>Semantic Understanding:</strong> 6/20 errors fixed (30%)</li>
                                <li><strong>One Step Missing:</strong> 12/18 errors fixed (67%)</li>
                                <li><strong>Other Errors:</strong> 4/7 errors fixed (57%)</li>
                            </ul>
                        </div>
                        
                        <p><strong>Important Note:</strong> Model scale is often conflated with amount of training compute, making it difficult to isolate the specific factors driving improvement.</p>
                    </div>
                </div>

                <div class="special-card" onclick="toggleSpecial('complexity')">
                    <h3>üé¢ Larger Gains for Complex Problems</h3>
                    <p>Chain-of-thought shows bigger improvements on harder reasoning tasks</p>
                    
                    <div class="special-details" id="complexity-details">
                        <div class="highlight">
                            <h4>üìä Performance by Problem Complexity</h4>
                            <ul>
                                <li><strong>GSM8K (most complex):</strong> Performance more than doubled for largest models</li>
                                <li><strong>Multi-step problems:</strong> Significant improvements across all benchmarks</li>
                                <li><strong>Single-step problems:</strong> Minimal or negative improvements</li>
                            </ul>
                        </div>
                        
                        <div class="results-section">
                            <h4>üìà GSM8K Performance Gains:</h4>
                            <ul>
                                <li><strong>GPT-3 175B:</strong> 15.6% ‚Üí 46.9% (+31.3%)</li>
                                <li><strong>PaLM 540B:</strong> 17.9% ‚Üí 56.9% (+39.0%)</li>
                                <li><strong>Codex:</strong> 19.7% ‚Üí 63.1% (+43.4%)</li>
                            </ul>
                        </div>
                        
                        <div class="success-analysis">
                            <h4>üí° Why Complex Problems Benefit More:</h4>
                            <ul>
                                <li><strong>Semantic Complexity:</strong> Hard to translate directly into equations</li>
                                <li><strong>Multi-step Nature:</strong> Benefits from intermediate reasoning</li>
                                <li><strong>Error Accumulation:</strong> Standard prompting compounds mistakes</li>
                                <li><strong>Reasoning Space:</strong> More computation allocated to harder problems</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script>
        function toggleCard(cardId) {
            const details = document.getElementById(cardId + '-details');
            const card = details.closest('.task-card');
            
            if (details.style.display === 'none' || details.style.display === '') {
                details.style.display = 'block';
                card.classList.add('expanded');
            } else {
                details.style.display = 'none';
                card.classList.remove('expanded');
            }
        }

        function toggleSpecial(cardId) {
            const details = document.getElementById(cardId + '-details');
            
            if (details.style.display === 'none' || details.style.display === '') {
                details.style.display = 'block';
            } else {
                details.style.display = 'none';
            }
        }

        // Add smooth scrolling for better UX
        document.querySelectorAll('.task-card, .special-card').forEach(card => {
            card.addEventListener('click', function() {
                setTimeout(() => {
                    this.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
                }, 300);
            });
        });
    </script>
</body>
</html>